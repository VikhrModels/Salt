train_batch_size: 1
eval_batch_size: 2
learning_rate: 1e-4
gradient_accumulation_steps: 1
lr_scheduler_type: "cosine"
num_train_epochs: 5
num_warmup_steps: 1000
weight_decay: 0.1
max_grad_norm: 0.25
torch_compile: False
save_total_limit: 7