base_model: "Qwen/Qwen2.5-0.5B"
checkpoint_path: null
save_dir: "./results"

start_sequence_token: "<|im_start|>"
end_sequence_token: "<|im_end|>"
start_audio_token: "<|start_of_audio|>"
end_audio_token: "<|end_of_audio|>"

n_special_tokens: 3

data:
  - Vikhrmodels/parler_tts_with_description_quantized-wav-uni
  - Vikhrmodels/emilia_multilang_quantized-wav-uni

text_data: []

max_seq_length: 2048
raw_audio_length: 256000

path_to_cache: ".."

allow_tf32: True

tasks:
  - asr
  - tts

# Quantizer settings
quantizer:
  speech:
    n_new_tokens: 0
  wav:
    n_new_tokens: 4096
  asr:
    - quantizer: wav
      n_codebooks: 1

  tts:
    - quantizer: wav
      n_codebooks: 1

# Training settings
train_batch_size: 16
eval_batch_size: 16
learning_rate: 1e-4
gradient_accumulation_steps: 2
lr_scheduler_type: "cosine"
num_train_epochs: 5
num_warmup_steps: 1000

weight_decay: 0.1
max_grad_norm: 0.25

# Logging settings
wandb_project_name: "vikhr4o-qwen2.5-0.5b"
