{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f13c7a3-6722-4da4-a696-65bc14a8a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e27ead1d-5193-4408-b534-fa13c23a4eba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from speechtokenizer import SpeechTokenizer\n",
    "from audiotools import AudioSignal\n",
    "\n",
    "# Параметры устройства\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Специальные токены\n",
    "start_audio_token = \"<soa>\"\n",
    "end_audio_token = \"<eoa>\"\n",
    "end_sequence_token = \"<eos>\"\n",
    "\n",
    "# Константы\n",
    "n_codebooks = 3\n",
    "max_seq_length = 1024\n",
    "top_k = 20\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset, Audio, Value\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    ")\n",
    "\n",
    "from speechtokenizer import SpeechTokenizer\n",
    "from audiotools import AudioSignal\n",
    "device = \"cuda:0\"\n",
    "n_special_tokens = 3\n",
    "model_path = \"Vikhrmodels/llama_asr_tts_24000\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=\".\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, cache_dir=\".\", attn_implementation=\"eager\", device_map={\"\":0})\n",
    "# Загрузка токенизатора и модели\n",
    "\n",
    "# Загрузка квантизатора\n",
    "config_path = \"../../audiotokenizer/speechtokenizer_hubert_avg_config.json\"\n",
    "ckpt_path = \"../../audiotokenizer/SpeechTokenizer.pt\"\n",
    "quantizer = SpeechTokenizer.load_from_checkpoint(config_path, ckpt_path)\n",
    "quantizer.eval()\n",
    "\n",
    "# Перемещение всех слоев квантизатора на устройство и их заморозка\n",
    "def freeze_entire_model(model):\n",
    "    for n, p in model.named_parameters():\n",
    "        p.requires_grad = False\n",
    "    return model\n",
    "\n",
    "for n, child in quantizer.named_children():\n",
    "    child.to(device)\n",
    "    child = freeze_entire_model(child)\n",
    "\n",
    "# Функция для создания токенов заполнения для аудио\n",
    "def get_audio_padding_tokens(quantizer):\n",
    "    audio = torch.zeros((1, 1, 1)).to(device)\n",
    "    codes = quantizer.encode(audio)\n",
    "    del audio\n",
    "    torch.cuda.empty_cache()\n",
    "    return {\"audio_tokens\": codes.squeeze(1)}\n",
    "\n",
    "# Функция для декодирования аудио из токенов\n",
    "def decode_audio(tokens, quantizer, pad_tokens, n_original_tokens):\n",
    "    start = torch.nonzero(tokens == tokenizer(start_audio_token)[\"input_ids\"][-1])\n",
    "    end = torch.nonzero(tokens == tokenizer(end_audio_token)[\"input_ids\"][-1])\n",
    "    start = start[0, -1] + 1 if len(start) else 0\n",
    "    end = end[0, -1] if len(end) else tokens.shape[-1]\n",
    "    \n",
    "    audio_tokens = tokens[start:end] % n_original_tokens\n",
    "    reminder = audio_tokens.shape[-1] % n_codebooks\n",
    "    \n",
    "    if reminder:\n",
    "        audio_tokens = torch.cat([audio_tokens, pad_tokens[reminder:n_codebooks]], dim=0)\n",
    "\n",
    "    transposed = audio_tokens.view(-1, n_codebooks).t()\n",
    "    codes = transposed.view(n_codebooks, 1, -1).to(device)\n",
    "    \n",
    "    audio = quantizer.decode(codes).squeeze(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    return AudioSignal(audio.detach().cpu().numpy(), quantizer.sample_rate)\n",
    "\n",
    "\n",
    "# Пример использования\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "baf391f1-e8aa-4cf5-8928-a8b45edb3bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция инференса для текста на входе и аудио на выходе\n",
    "def infer_text_to_audio(text, model, tokenizer, quantizer, max_seq_length=1024, top_k=20):\n",
    "    text_tokenized = tokenizer(text, return_tensors=\"pt\")\n",
    "    text_input_tokens = text_tokenized[\"input_ids\"].to(device)\n",
    "    \n",
    "    soa = tokenizer(start_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "    eoa = tokenizer(end_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "    \n",
    "    text_tokens = torch.cat([text_input_tokens, soa], dim=1)\n",
    "    attention_mask = torch.ones(text_tokens.size(), device=device)\n",
    "    \n",
    "    output_audio_tokens = model.generate(text_tokens, attention_mask=attention_mask, max_new_tokens=max_seq_length, top_k=top_k, do_sample=True)\n",
    "    \n",
    "    padding_tokens = get_audio_padding_tokens(quantizer)[\"audio_tokens\"].to(device)\n",
    "    audio_signal = decode_audio(output_audio_tokens[0], quantizer, padding_tokens.t()[0], len(tokenizer) - 1024)\n",
    "    \n",
    "    return audio_signal\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cd922f9-6cf1-498f-a180-ac9f409978a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция инференса для аудио на входе и текста на выходе\n",
    "def infer_audio_to_text(audio_path, model, tokenizer, quantizer, max_seq_length=1024, top_k=20):\n",
    "    audio_data, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    audio = audio_data.view(1, 1, -1).float().to(device)\n",
    "    codes = quantizer.encode(audio)\n",
    "    n_codebooks_a = 1\n",
    "    raw_audio_tokens = codes[:, :n_codebooks_a] + len(tokenizer) - 1024\n",
    "    \n",
    "    soa = tokenizer(start_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "    eoa = tokenizer(end_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "    audio_tokens = torch.cat([soa, raw_audio_tokens.view(1, -1), eoa], dim=1)\n",
    "    \n",
    "    attention_mask = torch.ones(audio_tokens.size(), device=device)\n",
    "    \n",
    "    output_text_tokens = model.generate(audio_tokens, attention_mask=attention_mask, max_new_tokens=max_seq_length, top_k=top_k, do_sample=True)\n",
    "    \n",
    "    output_text_tokens = output_text_tokens.cpu()[0]\n",
    "    output_text_tokens = output_text_tokens[output_text_tokens < tokenizer(start_audio_token)[\"input_ids\"][-1]]\n",
    "    decoded_text = tokenizer.decode(output_text_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d6f3272-952d-4cf6-b59b-8034facbab75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<audiotools.core.audio_signal.AudioSignal at 0x7fe04f104a10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Инференс текста в аудио\n",
    "text = \"Я хожу пешком.\".upper()\n",
    "audio_signal = infer_text_to_audio(text, model, tokenizer, quantizer)\n",
    "audio_signal.write(\"generated_audio_.wav\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2112d01-bed4-4ce3-b117-3f89fffffef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> THAN THIS NOTHAN SOLITUDE ALBERT YOU'LL BE IN COURSE WE'RE IN HERE AND THERE'S BUT FOR WITH A PHILIP OF WILL THERE WON'T FORGET WHAT'S THE NORTH RULE AH HOW CAN WE'LL BE BORN I'VE EVER THOUGHT IF YOU'RE BUT WHAT'S IT BUT A MATTER OF NONSENEW IF I'D BET THAT IS IT ANOTHER MAN NORA NORWAY TO IT IT'S RONICKY DOONE HE'S SOH NOW I'VE A ROSTOV MANAGED TO TRY HIM BACK ARE A RESOLUTION OF THIS STONE THAT'S SOME I'LL LET'S SEE HERE BETTER NOW\n"
     ]
    }
   ],
   "source": [
    "# Инференс аудио в текст\n",
    "audio_path = \"generated_audio.wav\"\n",
    "generated_text = infer_audio_to_text(audio_path, model, tokenizer, quantizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b51f0-e72d-409f-ad0e-e046b685b7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
