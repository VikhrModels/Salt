{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52fc6973-1181-40b7-9861-2417e069bcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/opt/conda/lib/python3.12/site-packages/speechtokenizer/model.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  params = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'vanilla' with 768 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/Experements/4o/WavTokenizer/decoder/pretrained.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict_raw = torch.load(model_path, map_location=\"cpu\")['state_dict']\n",
      "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:653: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.05 MB. The target location models--Vikhrmodels--salt-asr_speech_1_wav_1_tts_speech_3_instruct-8k/blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:653: UserWarning: Not enough free disk space to download the file. The expected file size is: 9.09 MB. The target location models--Vikhrmodels--salt-asr_speech_1_wav_1_tts_speech_3_instruct-8k/blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:653: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.00 MB. The target location models--Vikhrmodels--salt-asr_speech_1_wav_1_tts_speech_3_instruct-8k/blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:653: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.02 MB. The target location models--Vikhrmodels--salt-asr_speech_1_wav_1_tts_speech_3_instruct-8k/blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:653: UserWarning: Not enough free disk space to download the file. The expected file size is: 4997.32 MB. The target location models--Vikhrmodels--salt-asr_speech_1_wav_1_tts_speech_3_instruct-8k/blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n",
      "Downloading shards:  50%|█████     | 1/2 [01:59<01:59, 119.39s/it]/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:653: UserWarning: Not enough free disk space to download the file. The expected file size is: 1459.75 MB. The target location models--Vikhrmodels--salt-asr_speech_1_wav_1_tts_speech_3_instruct-8k/blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 2/2 [02:34<00:00, 77.33s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\n",
      "/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:653: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.00 MB. The target location models--Vikhrmodels--salt-asr_speech_1_wav_1_tts_speech_3_instruct-8k/blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://b23b00ddf17cbda4da.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b23b00ddf17cbda4da.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"WavTokenizer\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "import librosa\n",
    "\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "\n",
    "from speechtokenizer import SpeechTokenizer\n",
    "from WavTokenizer.decoder.pretrained import WavTokenizer\n",
    "from audiotools import AudioSignal\n",
    "\n",
    "\n",
    "def resample(audio_data: torch.Tensor, sample_rate: int):\n",
    "    print(\"Inout sample rate:\", sample_rate)\n",
    "    if sample_rate == 24000:\n",
    "      audio_data24k = audio_data\n",
    "      audio_data16k = torch.tensor(\n",
    "          librosa.resample(\n",
    "              audio_data.cpu().detach().numpy(), orig_sr=sample_rate, target_sr=16000\n",
    "          )\n",
    "      )\n",
    "    elif sample_rate == 16000:\n",
    "      audio_data16k = audio_data\n",
    "      audio_data24k = torch.tensor(\n",
    "          librosa.resample(\n",
    "              audio_data.cpu().detach().numpy(), orig_sr=sample_rate, target_sr=24000\n",
    "          )\n",
    "      )\n",
    "    else:\n",
    "      print(\"Resampling everything\")\n",
    "      audio_data16k = torch.tensor(\n",
    "          librosa.resample(\n",
    "              audio_data.cpu().detach().numpy(), orig_sr=sample_rate, target_sr=16000\n",
    "          )\n",
    "      )\n",
    "      audio_data24k = torch.tensor(\n",
    "          librosa.resample(\n",
    "              audio_data.cpu().detach().numpy(), orig_sr=sample_rate, target_sr=24000\n",
    "          )\n",
    "      )\n",
    "\n",
    "    return (audio_data16k.view(1, -1).float().to(device), \n",
    "           audio_data24k.view(1, -1).float().to(device))\n",
    "\n",
    "\n",
    "def decode_tts(tokens, quantizer, n_codebooks, n_original_tokens, start_audio_token_id, end_audio_token_id):\n",
    "    # find start and end indices of audio tokens\n",
    "    start = torch.nonzero(tokens == start_audio_token_id)\n",
    "    end = torch.nonzero(tokens == end_audio_token_id)\n",
    "\n",
    "    start = start[0, -1] + 1 if len(start) else 0\n",
    "    end = end[0, -1] if len(end) else tokens.shape[-1]\n",
    "\n",
    "    # subtract length of original vocabulary -> tokens in range [0, 1024)\n",
    "    audio_tokens = tokens[start:end] % n_original_tokens\n",
    "    reminder = audio_tokens.shape[-1] % n_codebooks\n",
    "\n",
    "    if reminder:\n",
    "        # pad if last frame is incomplete\n",
    "        pad_tokens = torch.zeros(n_codebooks - reminder, device=\"cuda\")\n",
    "        audio_tokens = torch.cat([audio_tokens, pad_tokens], dim=0)\n",
    "\n",
    "    transposed = audio_tokens.view(-1, n_codebooks).t()\n",
    "    codes = transposed.view(n_codebooks, 1, -1).to(device)\n",
    "\n",
    "    audio = quantizer.decode(codes).squeeze(0)\n",
    "\n",
    "    del tokens\n",
    "    del audio_tokens\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return AudioSignal(audio.detach().cpu().numpy(), quantizer.sample_rate)\n",
    "\n",
    "\n",
    "def infer_text_to_audio(text, model, tokenizer, quantizer, max_seq_length=1024, top_k=20):\n",
    "    text_tokenized = tokenizer(text, return_tensors=\"pt\")\n",
    "    text_input_tokens = text_tokenized[\"input_ids\"].to(device)\n",
    "\n",
    "    soa = tokenizer(start_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "    eoa = tokenizer(end_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "\n",
    "    text_tokens = torch.cat([text_input_tokens, soa], dim=1)\n",
    "    attention_mask = torch.ones(text_tokens.size(), device=device)\n",
    "\n",
    "    output_audio_tokens = model.generate(\n",
    "        text_tokens,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_seq_length,\n",
    "        top_k=top_k,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        repetition_penalty=1.1,\n",
    "        length_penalty=1.2,\n",
    "        no_repeat_ngram_size=3,\n",
    "    )\n",
    "\n",
    "    audio_signal = decode_tts(output_audio_tokens[0], quantizer, 3, len(tokenizer), soa, eoa)\n",
    "\n",
    "    return audio_signal\n",
    "\n",
    "\n",
    "def infer_audio_to_text(audio_path, model, tokenizer, quantizer_speech, quantizer_wav, max_seq_length=1024, top_k=20):\n",
    "    audio_data, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "    audio_16k, audio_24k = resample(audio_data, sample_rate)\n",
    "    bandwidth_id = torch.tensor([0])\n",
    "\n",
    "    codes_semantics = quantizer_speech.encode(audio_16k.reshape(1, 1, -1))\n",
    "    raw_semantic_tokens = codes_semantics + len(tokenizer)\n",
    "    raw_semantic_tokens = raw_semantic_tokens[:1].view(1, -1)\n",
    "\n",
    "    _, codes = quantizer_wav.encode_infer(audio_24k, bandwidth_id=bandwidth_id)\n",
    "    raw_acoustic_tokens = codes + len(tokenizer) + 1024\n",
    "    raw_acoustic_tokens = raw_acoustic_tokens.view(1, -1)\n",
    "\n",
    "    audio_tokens = torch.cat([raw_semantic_tokens, raw_acoustic_tokens], dim=1)\n",
    "\n",
    "    soa = tokenizer(start_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "    eoa = tokenizer(end_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "    audio_tokens = torch.cat([soa, audio_tokens, eoa], dim=1)\n",
    "    \n",
    "    # text_tokens = tokenizer(\"is said with\", return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "    tokens = torch.cat([audio_tokens], dim=1)\n",
    "\n",
    "    attention_mask = torch.ones(tokens.size(), device=device)\n",
    "\n",
    "    output_text_tokens = model.generate(\n",
    "        tokens,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_seq_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    output_text_tokens = output_text_tokens.cpu()[0]\n",
    "    output_text_tokens = output_text_tokens[output_text_tokens < tokenizer(start_audio_token)[\"input_ids\"][-1]]\n",
    "    decoded_text = tokenizer.decode(output_text_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return decoded_text\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "n_codebooks_tts = 3\n",
    "n_codebooks_asr = 1\n",
    "\n",
    "start_audio_token = \"<|start_of_audio|>\"\n",
    "end_audio_token = \"<|end_of_audio|>\"\n",
    "end_sequence_token = \"<|end_of_text|>\"\n",
    "\n",
    "base_model = \"Vikhrmodels/salt-asr_speech_1_wav_1_tts_speech_3_instruct-8k\"\n",
    "\n",
    "\n",
    "quantizer_speech = SpeechTokenizer.load_from_checkpoint(\"audiotokenizer/speechtokenizer_hubert_avg_config.json\",\n",
    "                                                        \"audiotokenizer/SpeechTokenizer.pt\")\n",
    "quantizer_speech = quantizer_speech.eval().to(device)\n",
    "codebook_size = quantizer_speech.quantizer.bins\n",
    "\n",
    "quantizer_wav = WavTokenizer.from_pretrained0802(\"WavTokenizer/configs/wavtokenizer_smalldata_frame40_3s_nq1_code4096_dim512_kmeans200_attn.yaml\",\n",
    "                                                 \"audiotokenizer/WavTokenizer_small_600_24k_4096.ckpt\")\n",
    "quantizer_wav = quantizer_wav.to(device)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, cache_dir=\".\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    cache_dir=\".\",\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "\n",
    "# Подключение функций\n",
    "def infer_text_to_audio(text, prompt, top_k=20, top_p=0.8, temperature=1):\n",
    "    # Форматирование текста с учетом шаблона и инструкций\n",
    "    max_seq_length=1024\n",
    "    formatted_text = f\"Say '{text.upper()}' {prompt}\"\n",
    "    \n",
    "    # Токенизация текста\n",
    "    text_tokenized = tokenizer(formatted_text, return_tensors=\"pt\")\n",
    "    text_input_tokens = text_tokenized[\"input_ids\"].to(device)\n",
    "    soa = tokenizer(start_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "    eoa = tokenizer(end_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "    text_tokens = torch.cat([text_input_tokens, soa], dim=1)\n",
    "    attention_mask = torch.ones(text_tokens.size(), device=device)\n",
    "\n",
    "    output_audio_tokens = model.generate(\n",
    "        text_tokens,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_seq_length,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        no_repeat_ngram_size=3,\n",
    "        #length_penalty=2.0,\n",
    "        repetition_penalty=1.5,\n",
    "    )\n",
    "\n",
    "    audio_signal = decode_tts(output_audio_tokens[0], quantizer_speech, 3, len(tokenizer), soa, eoa)\n",
    "    output_file = \"output_audio.wav\"\n",
    "    audio_signal.write(output_file)\n",
    "    return output_file\n",
    "\n",
    "def infer_audio_to_text(audio_path, max_seq_length=1024, top_k=200):\n",
    "    audio_data, sample_rate = torchaudio.load(audio_path)\n",
    "    audio = audio_data.view(1, -1).float().to(device)\n",
    "    bandwidth_id = torch.tensor([0])\n",
    "    codes_semantics = quantizer_speech.encode(audio.reshape(1, 1, -1))\n",
    "    raw_semantic_tokens = codes_semantics + len(tokenizer)\n",
    "    raw_semantic_tokens = raw_semantic_tokens[:1].view(1, -1)\n",
    "    _, codes = quantizer_wav.encode_infer(audio, bandwidth_id=bandwidth_id)\n",
    "    raw_acoustic_tokens = codes + len(tokenizer) + 1024\n",
    "    raw_acoustic_tokens = raw_acoustic_tokens.view(1, -1)\n",
    "    audio_tokens = torch.cat([raw_semantic_tokens, raw_acoustic_tokens], dim=1)\n",
    "    soa = tokenizer(start_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "    eoa = tokenizer(end_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "    audio_tokens = torch.cat([soa, audio_tokens, eoa], dim=1)\n",
    "    tokens = torch.cat([audio_tokens], dim=1)\n",
    "    attention_mask = torch.ones(tokens.size(), device=device)\n",
    "\n",
    "    output_text_tokens = model.generate(\n",
    "        tokens,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_seq_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    output_text_tokens = output_text_tokens.cpu()[0]\n",
    "    output_text_tokens = output_text_tokens[output_text_tokens < tokenizer(start_audio_token)[\"input_ids\"][-1]]\n",
    "    decoded_text = tokenizer.decode(output_text_tokens, skip_special_tokens=True)\n",
    "    return decoded_text\n",
    "\n",
    "# Интерфейс Gradio\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Text-to-Audio and Audio-to-Text Conversion\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        text_input = gr.Textbox(label=\"Text to Say\", placeholder=\"Enter the text to be spoken, e.g., 'Hello everyone'\")\n",
    "        prompt_input = gr.Textbox(\n",
    "            label=\"Voice Instructions\", \n",
    "            placeholder=\n",
    "                (\"with a female voice: lively, expressive, with a playful and energetic tone. The voice should be dynamic and slightly high-pitched, conveying excitement and charm. Ensure the recording is clear and crisp, with minimal background noise.\")\n",
    "            \n",
    "        )\n",
    "        audio_output = gr.Audio(label=\"Generated Audio\", type=\"filepath\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        # Крутилки для управления параметрами\n",
    "        top_k_slider = gr.Slider(1, 200, value=20, step=1, label=\"Top-k\")\n",
    "        top_p_slider = gr.Slider(0.0, 1.0, value=0.8, step=0.01, label=\"Top-p\")\n",
    "        temperature_slider = gr.Slider(0.0, 2.0, value=1.0, step=0.01, label=\"Temperature\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        gr.Markdown(\"### Generate Audio from Text and Instructions\")\n",
    "        text_to_audio_button = gr.Button(\"Generate Audio\")\n",
    "        text_to_audio_button.click(\n",
    "            fn=infer_text_to_audio, \n",
    "            inputs=[text_input, prompt_input, top_k_slider, top_p_slider, temperature_slider], \n",
    "            outputs=audio_output\n",
    "        )\n",
    "    \n",
    "    with gr.Row():\n",
    "        audio_input = gr.Audio(label=\"Input Audio for Text Generation\", type=\"filepath\")\n",
    "        text_output = gr.Textbox(label=\"Generated Text from Audio\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        gr.Markdown(\"### Generate Text from Audio\")\n",
    "        audio_to_text_button = gr.Button(\"Generate Text\")\n",
    "        audio_to_text_button.click(\n",
    "            fn=infer_audio_to_text, \n",
    "            inputs=[audio_input], \n",
    "            outputs=text_output\n",
    "        )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8246ceac-3060-4109-8edd-9b6d1af6df3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "overlay         234G  185G   38G  84% /\n",
      "tmpfs            64M     0   64M   0% /dev\n",
      "tmpfs           126G     0  126G   0% /sys/fs/cgroup\n",
      "shm             8.0G  4.0K  8.0G   1% /dev/shm\n",
      "/dev/fioa1      2.9T  2.8T     0 100% /app\n",
      "/dev/sda1        15T   14T  1.3T  92% /mnt/storage\n",
      "/dev/sdb2       234G  185G   38G  84% /etc/hosts\n",
      "tmpfs           126G   12K  126G   1% /proc/driver/nvidia\n",
      "udev            126G     0  126G   0% /dev/nvidia0\n",
      "tmpfs           126G     0  126G   0% /proc/acpi\n",
      "tmpfs           126G     0  126G   0% /proc/scsi\n",
      "tmpfs           126G     0  126G   0% /sys/firmware\n",
      "tmpfs           126G     0  126G   0% /sys/devices/virtual/powercap\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7127660-59a5-4a09-a066-78e4cb49e21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19c4163c-150e-4de4-9d5b-5a9a78ab0dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://29bface89ef8bdf109.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://29bface89ef8bdf109.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6547c229-e8b0-4cf3-809d-e98b6cd6db7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://a53a64c8e6b2da3fc8.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a53a64c8e6b2da3fc8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "\n",
    "# Подключение функций\n",
    "def infer_text_to_audio(formatted_text, max_seq_length=1024, top_k=20, top_p=0.8, temperature=1):\n",
    "    # Токенизация текста\n",
    "    text_tokenized = tokenizer(formatted_text, return_tensors=\"pt\")\n",
    "    text_input_tokens = text_tokenized[\"input_ids\"].to(device)\n",
    "    soa = tokenizer(start_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "    eoa = tokenizer(end_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "    text_tokens = torch.cat([text_input_tokens, soa], dim=1)\n",
    "    attention_mask = torch.ones(text_tokens.size(), device=device)\n",
    "\n",
    "    output_audio_tokens = model.generate(\n",
    "        text_tokens,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_seq_length,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        no_repeat_ngram_size=3,\n",
    "    )\n",
    "\n",
    "    audio_signal = decode_tts(output_audio_tokens[0], quantizer_speech, 3, len(tokenizer), soa, eoa)\n",
    "    output_file = \"output_audio.wav\"\n",
    "    audio_signal.write(output_file)\n",
    "    return output_file\n",
    "\n",
    "def infer_audio_to_text(audio_path, max_seq_length=1024, top_k=200):\n",
    "    audio_data, sample_rate = torchaudio.load(audio_path)\n",
    "    audio = audio_data.view(1, -1).float().to(device)\n",
    "    bandwidth_id = torch.tensor([0])\n",
    "    codes_semantics = quantizer_speech.encode(audio.reshape(1, 1, -1))\n",
    "    raw_semantic_tokens = codes_semantics + len(tokenizer)\n",
    "    raw_semantic_tokens = raw_semantic_tokens[:1].view(1, -1)\n",
    "    _, codes = quantizer_wav.encode_infer(audio, bandwidth_id=bandwidth_id)\n",
    "    raw_acoustic_tokens = codes + len(tokenizer) + 1024\n",
    "    raw_acoustic_tokens = raw_acoustic_tokens.view(1, -1)\n",
    "    audio_tokens = torch.cat([raw_semantic_tokens, raw_acoustic_tokens], dim=1)\n",
    "    soa = tokenizer(start_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "    eoa = tokenizer(end_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "    audio_tokens = torch.cat([soa, audio_tokens, eoa], dim=1)\n",
    "    tokens = torch.cat([audio_tokens], dim=1)\n",
    "    attention_mask = torch.ones(tokens.size(), device=device)\n",
    "\n",
    "    output_text_tokens = model.generate(\n",
    "        tokens,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_seq_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.01,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    output_text_tokens = output_text_tokens.cpu()[0]\n",
    "    output_text_tokens = output_text_tokens[output_text_tokens < tokenizer(start_audio_token)[\"input_ids\"][-1]]\n",
    "    decoded_text = tokenizer.decode(output_text_tokens, skip_special_tokens=True)\n",
    "    return decoded_text\n",
    "\n",
    "# Интерфейс Gradio\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Text-to-Audio and Audio-to-Text Conversion\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        formatted_text_input = gr.Textbox(\n",
    "            label=\"Input Formatted Text for Audio Generation\", \n",
    "            placeholder=(\n",
    "                \"Examples:\\n\"\n",
    "                \"1. Say 'HELLO EVERYONE' with a cheerful and energetic voice. The tone should be friendly and welcoming.\\n\"\n",
    "                \"2. Say 'WELCOME TO THE PARTY' with an enthusiastic and lively tone, like an invitation to a fun event.\\n\"\n",
    "                \"3. Say 'GOOD MORNING' with a calm and soothing female voice, warm and reassuring.\\n\"\n",
    "                \"4. Say 'LET'S LEARN TOGETHER' with a confident and clear voice for an educational setting.\\n\"\n",
    "                \"5. Say 'THIS IS A TEST OF THE EMERGENCY SYSTEM' with a serious and authoritative tone.\"\n",
    "            )\n",
    "        )\n",
    "        audio_output = gr.Audio(label=\"Generated Audio\", type=\"filepath\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        gr.Markdown(\"### Generate Audio from Formatted Text\")\n",
    "        text_to_audio_button = gr.Button(\"Generate Audio\")\n",
    "        text_to_audio_button.click(\n",
    "            fn=infer_text_to_audio, \n",
    "            inputs=[formatted_text_input], \n",
    "            outputs=audio_output\n",
    "        )\n",
    "    \n",
    "    with gr.Row():\n",
    "        audio_input = gr.Audio(label=\"Input Audio for Text Generation\", type=\"filepath\")\n",
    "        text_output = gr.Textbox(label=\"Generated Text from Audio\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        gr.Markdown(\"### Generate Text from Audio\")\n",
    "        audio_to_text_button = gr.Button(\"Generate Text\")\n",
    "        audio_to_text_button.click(\n",
    "            fn=infer_audio_to_text, \n",
    "            inputs=[audio_input], \n",
    "            outputs=text_output\n",
    "        )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b834584c-946a-421d-8079-492804a72b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(133379, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=133379, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ae87647f-221e-47a5-932b-1d35bc052427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 37429, done.\u001b[K\n",
      "remote: Counting objects: 100% (9303/9303), done.\u001b[K\n",
      "remote: Compressing objects: 100% (876/876), done.\u001b[K\n",
      "remote: Total 37429 (delta 8860), reused 8687 (delta 8420), pack-reused 28126 (from 1)\u001b[K\n",
      "Receiving objects: 100% (37429/37429), 60.56 MiB | 24.89 MiB/s, done.\n",
      "Resolving deltas: 100% (27289/27289), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8d86c0c9-8def-49c5-bc71-34194051cca8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: numpy~=1.26.4 in /opt/conda/lib/python3.12/site-packages (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: sentencepiece~=0.2.0 in /opt/conda/lib/python3.12/site-packages (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.45.1 in /opt/conda/lib/python3.12/site-packages (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.46.2)\n",
      "Requirement already satisfied: gguf>=0.1.0 in /opt/conda/lib/python3.12/site-packages (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /opt/conda/lib/python3.12/site-packages (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 5)) (4.25.5)\n",
      "Collecting torch~=2.2.1 (from -r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3))\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.2.2%2Bcpu-cp312-cp312-linux_x86_64.whl (186.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.7/186.7 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.12/site-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.12/site-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy->torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.0\n",
      "    Uninstalling torch-2.4.0:\n",
      "      Successfully uninstalled torch-2.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xformers 0.0.27.post2 requires torch==2.4.0, but you have torch 2.2.2+cpu which is incompatible.\n",
      "torchvision 0.19.0 requires torch==2.4.0, but you have torch 2.2.2+cpu which is incompatible.\n",
      "torchaudio 2.4.0 requires torch==2.4.0, but you have torch 2.2.2+cpu which is incompatible.\n",
      "vllm 0.6.3.post1 requires torch==2.4.0, but you have torch 2.2.2+cpu which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.2.2+cpu\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f9c45fd6-5fc7-48b5-baec-9e25a291fe45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: convert_hf_to_gguf.py [-h] [--vocab-only] [--outfile OUTFILE]\n",
      "                             [--outtype {f32,f16,bf16,q8_0,tq1_0,tq2_0,auto}]\n",
      "                             [--bigendian] [--use-temp-file] [--no-lazy]\n",
      "                             [--model-name MODEL_NAME] [--verbose]\n",
      "                             [--split-max-tensors SPLIT_MAX_TENSORS]\n",
      "                             [--split-max-size SPLIT_MAX_SIZE] [--dry-run]\n",
      "                             [--no-tensor-first-split] [--metadata METADATA]\n",
      "                             model\n",
      "\n",
      "Convert a huggingface model to a GGML compatible file\n",
      "\n",
      "positional arguments:\n",
      "  model                 directory containing model file\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --vocab-only          extract only the vocab\n",
      "  --outfile OUTFILE     path to write to; default: based on input. {ftype}\n",
      "                        will be replaced by the outtype.\n",
      "  --outtype {f32,f16,bf16,q8_0,tq1_0,tq2_0,auto}\n",
      "                        output format - use f32 for float32, f16 for float16,\n",
      "                        bf16 for bfloat16, q8_0 for Q8_0, tq1_0 or tq2_0 for\n",
      "                        ternary, and auto for the highest-fidelity 16-bit\n",
      "                        float type depending on the first loaded tensor type\n",
      "  --bigendian           model is executed on big endian machine\n",
      "  --use-temp-file       use the tempfile library while processing (helpful\n",
      "                        when running out of memory, process killed)\n",
      "  --no-lazy             use more RAM by computing all outputs before writing\n",
      "                        (use in case lazy evaluation is broken)\n",
      "  --model-name MODEL_NAME\n",
      "                        name of the model\n",
      "  --verbose             increase output verbosity\n",
      "  --split-max-tensors SPLIT_MAX_TENSORS\n",
      "                        max tensors in each split\n",
      "  --split-max-size SPLIT_MAX_SIZE\n",
      "                        max size per split N(M|G)\n",
      "  --dry-run             only print out a split plan and exit, without writing\n",
      "                        any new files\n",
      "  --no-tensor-first-split\n",
      "                        do not add tensors to the first split (disabled by\n",
      "                        default)\n",
      "  --metadata METADATA   Specify the path for an authorship metadata override\n",
      "                        file\n"
     ]
    }
   ],
   "source": [
    "!python3 llama.cpp/convert_hf_to_gguf.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c95e6304-9df8-4b5c-81c8-c4c11d692be1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: salt_a1\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> Q8_0, shape = {3072, 133379}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> Q8_0, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> Q8_0, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> Q8_0, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 24\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 7\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128001\n",
      "INFO:gguf.vocab:Setting special token type pad to 128256\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:salt-asr_speech_1_wav_1_tts_speech_3.gguf: n_tensors = 255, total_size = 3.4G\n",
      "Writing: 100%|██████████████████████████| 3.43G/3.43G [00:49<00:00, 69.8Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to salt-asr_speech_1_wav_1_tts_speech_3.gguf\n"
     ]
    }
   ],
   "source": [
    "!python llama.cpp/convert_hf_to_gguf.py salt_a1 \\\n",
    "  --outfile salt-asr_speech_1_wav_1_tts_speech_3.gguf \\\n",
    "  --outtype q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "55f3fee3-b358-4262-8187-45b28681a170",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "(Request ID: Root=1-67350a88-41549e911d83527e4af124e7;9edd5758-b40a-4194-a771-695cae7cc589)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"Vikhrmodels\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[186], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m api \u001b[38;5;241m=\u001b[39m HfApi()\n\u001b[1;32m      4\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVikhrmodels/salt_asr_speech_1_wav_1_tts_speech_3_gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m api\u001b[38;5;241m.\u001b[39mupload_file(\n\u001b[1;32m      7\u001b[0m     path_or_fileobj\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msalt-asr_speech_1_wav_1_tts_speech_3.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     path_in_repo\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msalt-asr_speech_1_wav_1_tts_speech_3.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m     10\u001b[0m     token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_rIVEYywWJmqdfeYhAJhaSDhiXukAoAqaqg\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/huggingface_hub/hf_api.py:3544\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3542\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RepoUrl(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3543\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m HfHubHTTPError:\n\u001b[0;32m-> 3544\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m   3545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3546\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/huggingface_hub/hf_api.py:3531\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3528\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   3530\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3531\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3532\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   3533\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[1;32m   3534\u001b[0m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:468\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    463\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure your token has the correct permissions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m     )\n\u001b[0;32m--> 468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m416\u001b[39m:\n\u001b[1;32m    471\u001b[0m     range_header \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: (Request ID: Root=1-67350a88-41549e911d83527e4af124e7;9edd5758-b40a-4194-a771-695cae7cc589)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"Vikhrmodels\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "model_id = \"Vikhrmodels/salt_asr_speech_1_wav_1_tts_speech_3_gguf\"\n",
    "api.create_repo(model_id, exist_ok=True, repo_type=\"model\")\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"salt-asr_speech_1_wav_1_tts_speech_3.gguf\",\n",
    "    path_in_repo=\"salt-asr_speech_1_wav_1_tts_speech_3.gguf\",\n",
    "    repo_id=model_id,\n",
    "    token='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',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b4f37-5008-4286-873d-cf11e77cb409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
