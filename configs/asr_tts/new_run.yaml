base_model: "Qwen/Qwen3-Embedding-0.6B"
#checkpoint_path: "./results/checkpoint-657000"
save_dir: "./experiments_results"

asr_data: []
  # - Vikhrmodels/librispeech_quantized-bigcodec
  # - Vikhrmodels/parler_tts_with_description_quantized
  # - Vikhrmodels/emilia_quantized-v1

tts_data:
  - Vikhrmodels/ToneSpeak_quantized-bigcodec
  - Vikhrmodels/audiobooks_quantized-bigcodec
  - Vikhrmodels/mozilla_slavic_quantized-bigcodec
  - Vikhrmodels/librispeech_quantized-bigcodec
  - Vikhrmodels/ToneBooks_quantized-bigcodec

text_data: []


start_sequence_token: "<|im_start|>"
end_sequence_token: "<|im_end|>"
start_audio_token: "<|start_of_audio|>"
end_audio_token: "<|end_of_audio|>"

n_special_tokens: 3

max_seq_length: 4096
filter_long_audio: False

path_to_cache: "./cache_home"

allow_tf32: True

# Freezing settings
freeze:
    freeze_emb: False
    freeze_ln: False
    freeze_attn: False
    freeze_ff: False
    freeze_ff_layers: False
      # - 5
      # - 6
      # - 7
      # - 8
      # - 9
      # - 12
      # - 23
      # - 14
      # - 18
      # - 19
      # - 20
      # - 0
      # - 25

    freeze_other: False

tasks:
  # - asr
  - tts

# Quantizer settings
quantizer:
  speech:
    n_new_tokens: 1024
  wav:
    n_new_tokens: 0
  bigcodec:
    n_new_tokens: 8192
  asr: []
   # - quantizer: bigcodec
   #   n_codebooks: 1

  tts:
    - quantizer: bigcodec
      n_codebooks: 1

# Training settings
train_batch_size: 1
eval_batch_size: 2
learning_rate: 2e-4
gradient_accumulation_steps: 1
lr_scheduler_type: "cosine"
num_train_epochs: 7
num_warmup_steps: 1000

weight_decay: 0.1
max_grad_norm: 0.25

torch_compile: False
save_total_limit: 7

# Logging settings
wandb_project_name: "asr-tts"
