{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f13c7a3-6722-4da4-a696-65bc14a8a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a4ec37-1f4a-45b1-9f2a-42ed82041c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/4o/lib/python3.12/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_from_disk, Audio\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, PeftModelForCausalLM\n",
    "\n",
    "import dac\n",
    "from audiotools import AudioSignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a31c3da7-4cc1-421f-8778-c9eba23272ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5885f68a085d493798642f4bbfbf979f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(257025, 2304, padding_idx=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"results2/checkpoint-8000\"\n",
    "base_model = \"google/gemma-2-2b\"\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=\".\")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, cache_dir=\".\", attn_implementation=\"eager\", device_map={\"\":0})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46589945-1864-44d5-8e7e-b8e49d6a8709",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = PeftModelForCausalLM.from_pretrained(model, model_path, adapter_name=\"lora\", peft_config=lora_config)\n",
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60e7de30-6ec2-4cb9-9b23-c8d53472e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.start_audio_token = tokenizer(start_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, 1:]\n",
    "        self.end_audio_token = tokenizer(end_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, 1:]\n",
    "        self.end_frame_token = tokenizer(end_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, 1:].item()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset[idx]\n",
    "        text = row[\"text\"]\n",
    "        text_tokenized = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        text_input_tokens = text_tokenized[\"input_ids\"]\n",
    "        \n",
    "        raw_audio_tokens = torch.tensor(row[\"audio_tokens\"])[:, :n_codebooks]\n",
    "        n_frames = raw_audio_tokens.shape[-1]\n",
    "        raw_audio_tokens = torch.cat([raw_audio_tokens, torch.full((1, 1, n_frames), self.end_frame_token)], dim=1)\n",
    "        \n",
    "        # permute: (n_codebooks, n_frames) -> (n_frames, n_codebooks)\n",
    "        audio_input_tokens = raw_audio_tokens.permute(2, 0, 1).contiguous().view(1, -1)\n",
    "\n",
    "        audio_length = min(max_seq_length - text_input_tokens.shape[-1] - 2, audio_input_tokens.shape[-1])\n",
    "        audio_length -= audio_length % n_codebooks\n",
    "        \n",
    "        input_tokens = torch.cat([text_input_tokens, self.start_audio_token], dim=1)\n",
    "        labels = torch.cat([audio_input_tokens[:, :audio_length], self.end_audio_token], dim=1)\n",
    "        attention_mask = torch.ones(input_tokens.shape)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_tokens, \n",
    "            \"attention_mask\": attention_mask, \n",
    "            \"labels\": labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88ea99e2-a79b-4268-bc1b-e6bde6ab4fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_padding_tokens(quantizer, n_original_tokens):\n",
    "    # create audio without any sounds \n",
    "    # seems to work better than radom padding if \n",
    "    # length of generated audio is not devisible by n_codebooks\n",
    "    audio = torch.zeros((1, 1, 1))\n",
    "    audio = audio.to(quantizer.device)\n",
    "    \n",
    "    x = quantizer.preprocess(audio, quantizer.sample_rate)\n",
    "    _, codes, _, _, _ = quantizer.encode(x)\n",
    "\n",
    "    # Move tensor back to CPU and delete it to free GPU memory\n",
    "    del audio\n",
    "    del x\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\"audio_tokens\": codes[0].t()}\n",
    "    \n",
    "\n",
    "\n",
    "def decode_audio(tokens, quantizer, pad_tokens, n_original_tokens):\n",
    "    # find start and end indices of audio tokens \n",
    "    tokens[tokens != end_frame_token_id]\n",
    "    start = torch.nonzero(tokens[0] == start_audio_token_id)\n",
    "    end = torch.nonzero(tokens[0] == tokenizer.eos_token_id)\n",
    "    \n",
    "    start = start[0, -1] + 1 if len(start) else 0\n",
    "    end = end[0, -1] if len(end) else tokens.shape[-1]\n",
    "    # substract length of original vocabulary -> tokens in range [0, 1024)\n",
    "    audio_tokens = tokens[:, start:end] % n_original_tokens\n",
    "    reminder = audio_tokens.shape[-1] % n_codebooks\n",
    "    \n",
    "    if reminder:\n",
    "        # pad if last frame is incomplete \n",
    "        audio_tokens = torch.cat([audio_tokens, pad_tokens[:, reminder:]], dim=1)\n",
    "\n",
    "    codes = audio_tokens.view(1, -1, n_codebooks).permute(0, 2, 1).to(quantizer.device)\n",
    "    z = quantizer.quantizer.from_codes(codes)[0]\n",
    "    audio = quantizer.decode(z)\n",
    "\n",
    "    del tokens \n",
    "    del audio_tokens \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return AudioSignal(audio.detach().cpu().numpy(), quantizer.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee784812-cac8-444e-bda3-e2a889770ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/4o/lib/python3.12/site-packages/audiotools/ml/layers/base.py:172: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_dict = torch.load(location, \"cpu\")\n",
      "/opt/conda/envs/4o/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "n_codebooks = 4\n",
    "quant_path = dac.utils.download(model_type=\"16khz\")\n",
    "quantizer = dac.DAC.load(quant_path, n_codebooks=n_codebooks).to(f\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "740248c9-75d2-4b24-815c-1c5be51a3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_audio_token = \"<soa>\"\n",
    "end_audio_token = \"<eos>\"\n",
    "end_frame_token = \"<eof>\"\n",
    "n_tokens = 256000\n",
    "\n",
    "val_data = load_from_disk(\"./data/processed/val\")\n",
    "val_dataset = TestDataset(val_data, tokenizer)\n",
    "\n",
    "padding_tokens = get_audio_padding_tokens(quantizer, n_tokens + 1)[\"audio_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e445b21-1128-496a-8e02-70cb2a561055",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 5\n",
    "start_audio_token_id = 256000\n",
    "end_frame_token_id = tokenizer(end_frame_token)[\"input_ids\"][-1]\n",
    "\n",
    "for i in range(n_examples):\n",
    "    row = val_dataset[i]\n",
    "\n",
    "    for k, v in row.items():\n",
    "        row[k] = v.to(\"cuda:0\")\n",
    "        \n",
    "    output = merged_model.generate(**row, max_new_tokens=max_seq_length)\n",
    "    audio = decode_audio(output.cpu(), quantizer, padding_tokens, n_tokens + 1)\n",
    "    audio.write(f\"tests/audio_{i}.wav\")\n",
    "    \n",
    "    print(tokenizer.decode(row[\"input_ids\"][0], skip_special_tokens=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cf0df3-25d9-46a1-998b-3acdb366b87d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (4o)",
   "language": "python",
   "name": "4o"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
