{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D7gU4k97rkvM",
    "outputId": "2b8af585-9904-4be9-da50-b61a57b17723"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'BigCodec'...\n",
      "remote: Enumerating objects: 72, done.\u001B[K\n",
      "remote: Counting objects: 100% (72/72), done.\u001B[K\n",
      "remote: Compressing objects: 100% (60/60), done.\u001B[K\n",
      "remote: Total 72 (delta 14), reused 57 (delta 7), pack-reused 0 (from 0)\u001B[K\n",
      "Receiving objects: 100% (72/72), 30.19 KiB | 572.00 KiB/s, done.\n",
      "Resolving deltas: 100% (14/14), done.\n",
      "--2025-03-28 12:17:15--  https://huggingface.co/Alethia/BigCodec/resolve/main/bigcodec.pt\n",
      "Resolving huggingface.co (huggingface.co)... 18.164.174.17, 18.164.174.23, 18.164.174.55, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.164.174.17|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.hf.co/repos/2e/d1/2ed1c2f0d3a6eeb0584a3dd54200d652d17330f9d54e66392623e9d0521364e3/1fba3806e87cc01c1a65bea22fa1becefbbf46881e4219593c4d9f3cf56206b9?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27bigcodec.pt%3B+filename%3D%22bigcodec.pt%22%3B&Expires=1743167835&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzE2NzgzNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJlL2QxLzJlZDFjMmYwZDNhNmVlYjA1ODRhM2RkNTQyMDBkNjUyZDE3MzMwZjlkNTRlNjYzOTI2MjNlOWQwNTIxMzY0ZTMvMWZiYTM4MDZlODdjYzAxYzFhNjViZWEyMmZhMWJlY2VmYmJmNDY4ODFlNDIxOTU5M2M0ZDlmM2NmNTYyMDZiOT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Gdl4Znia9Y-p0zgdRzstAXeAhNeGXYm7PJ-oj1%7Ee7%7EMitjaoM9j3mqMb07n5w6pLE77%7ELwww-F3bH0jg9BlI5TgUdJVW2B1XFk4ICQpXoP%7EYMisO6aImbGDZ1C-ErDuLy20g8-bdrEKhSBPAH5%7EzJUSZlV4PoauJHPCeR-JMRTE%7Es7P1UzdGtPIVUCUnZWLkZ-WpRRHaqSK9bKgqKSotn0yqfr6QGPB4nqx7UyYf-KZmm5PTl8JIOeFXQqwvZJ7DeQxM8V-2%7EYMyvNhpMaQuSzzJeYlPvdU9cUdrpoww12GfypcgjrQDyEOHfArxRTL%7ExmKqjP%7EY5kKuYNnS74Bcsw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
      "--2025-03-28 12:17:15--  https://cdn-lfs-us-1.hf.co/repos/2e/d1/2ed1c2f0d3a6eeb0584a3dd54200d652d17330f9d54e66392623e9d0521364e3/1fba3806e87cc01c1a65bea22fa1becefbbf46881e4219593c4d9f3cf56206b9?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27bigcodec.pt%3B+filename%3D%22bigcodec.pt%22%3B&Expires=1743167835&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzE2NzgzNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJlL2QxLzJlZDFjMmYwZDNhNmVlYjA1ODRhM2RkNTQyMDBkNjUyZDE3MzMwZjlkNTRlNjYzOTI2MjNlOWQwNTIxMzY0ZTMvMWZiYTM4MDZlODdjYzAxYzFhNjViZWEyMmZhMWJlY2VmYmJmNDY4ODFlNDIxOTU5M2M0ZDlmM2NmNTYyMDZiOT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Gdl4Znia9Y-p0zgdRzstAXeAhNeGXYm7PJ-oj1%7Ee7%7EMitjaoM9j3mqMb07n5w6pLE77%7ELwww-F3bH0jg9BlI5TgUdJVW2B1XFk4ICQpXoP%7EYMisO6aImbGDZ1C-ErDuLy20g8-bdrEKhSBPAH5%7EzJUSZlV4PoauJHPCeR-JMRTE%7Es7P1UzdGtPIVUCUnZWLkZ-WpRRHaqSK9bKgqKSotn0yqfr6QGPB4nqx7UyYf-KZmm5PTl8JIOeFXQqwvZJ7DeQxM8V-2%7EYMyvNhpMaQuSzzJeYlPvdU9cUdrpoww12GfypcgjrQDyEOHfArxRTL%7ExmKqjP%7EY5kKuYNnS74Bcsw__&Key-Pair-Id=K24J24Z295AEI9\n",
      "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 13.33.21.64, 13.33.21.11, 13.33.21.45, ...\n",
      "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|13.33.21.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 637937326 (608M) [binary/octet-stream]\n",
      "Saving to: ‘bigcodec.pt’\n",
      "\n",
      "bigcodec.pt         100%[===================>] 608.38M  50.1MB/s    in 12s     \n",
      "\n",
      "2025-03-28 12:17:28 (49.4 MB/s) - ‘bigcodec.pt’ saved [637937326/637937326]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Aria-K-Alethia/BigCodec.git\n",
    "!wget https://huggingface.co/Alethia/BigCodec/resolve/main/bigcodec.pt"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"BigCodec\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import librosa\n",
    "\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "token = \"\"\n",
    "base_model = 'ksych/salt-asr-0.5b'\n",
    "\n",
    "start_sequence_token = \"<|im_start|>\"\n",
    "end_sequence_token = \"<|im_end|>\"\n",
    "start_audio_token = \"<|start_of_audio|>\"\n",
    "end_audio_token = \"<|end_of_audio|>\"\n",
    "\n",
    "\n",
    "def resample(audio_data: torch.Tensor, sample_rate: int):\n",
    "    print(\"Inout sample rate:\", sample_rate)\n",
    "    if sample_rate != 16000:\n",
    "      audio_data = torch.tensor(\n",
    "          librosa.resample(\n",
    "              audio_data.cpu().detach().numpy(), orig_sr=sample_rate, target_sr=16000\n",
    "          )\n",
    "      )\n",
    "\n",
    "    return audio_data.view(1, -1).float().to(device)\n",
    "\n",
    "\n",
    "def get_audio_start_end_tokens(\n",
    "    tokens: torch.Tensor,\n",
    "    start_audio_token_id=None,\n",
    "    end_audio_token_id=None,\n",
    "):\n",
    "    # find start index of audio tokens\n",
    "    if start_audio_token_id is not None:\n",
    "        start = torch.nonzero(tokens == start_audio_token_id)\n",
    "        start = start[0, -1] + 1 if len(start) else 0\n",
    "    else:\n",
    "        start = 0\n",
    "\n",
    "    # find end index of audio tokens\n",
    "    if end_audio_token_id is not None:\n",
    "        end = torch.nonzero(tokens == end_audio_token_id)\n",
    "        end = end[0, -1] if len(end) else tokens.shape[-1]\n",
    "    else:\n",
    "        end = tokens.shape[-1]\n",
    "\n",
    "    assert start < end, (\n",
    "        f\"Start of audio must be before end. Found: start - {start}, end - {end}\"\n",
    "    )\n",
    "\n",
    "    return start, end\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, cache_dir=\".\", use_auth_token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    cache_dir=\".\",\n",
    "    attn_implementation=\"sdpa\",\n",
    "    device_map={\"\": 0},\n",
    "    use_auth_token=token\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HbpGDIkqrz8m",
    "outputId": "5142ec81-2e9a-43ab-a205-b61dc6911b9e"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:862: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "bos_id = tokenizer.convert_tokens_to_ids(start_sequence_token)\n",
    "soa_id = tokenizer.convert_tokens_to_ids(start_audio_token)\n",
    "eoa_id = tokenizer.convert_tokens_to_ids(end_audio_token)\n",
    "\n",
    "bos = torch.tensor([[bos_id]], device=device)\n",
    "soa = torch.tensor([[soa_id]], device=device)\n",
    "eoa = torch.tensor([[eoa_id]], device=device)"
   ],
   "metadata": {
    "id": "ol06xXn4u5oL"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from BigCodec.vq.codec_encoder import CodecEncoder\n",
    "from BigCodec.vq.codec_decoder import CodecDecoder\n",
    "\n",
    "\n",
    "class BigCodecTokenizer:\n",
    "    def __init__(self, ckpt_path):\n",
    "        ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        encoder = CodecEncoder()\n",
    "        encoder.load_state_dict(ckpt[\"CodecEnc\"])\n",
    "        self.encoder = encoder.eval().cuda()\n",
    "\n",
    "        decoder = CodecDecoder()\n",
    "        decoder.load_state_dict(ckpt[\"generator\"])\n",
    "        self.decoder = decoder.eval().cuda()\n",
    "\n",
    "    def encode(self, wav):\n",
    "        vq_emb = self.encoder(wav.unsqueeze(1))\n",
    "        _, vq_code, _ = self.decoder(vq_emb, vq=True)\n",
    "        return vq_code\n",
    "\n",
    "\n",
    "n_codebooks_tts = 1\n",
    "n_codebooks_asr = 1\n",
    "quantizer = BigCodecTokenizer(\"bigcodec.pt\")\n",
    "n_audio_tokens = quantizer.decoder.quantizer.layers[0].codebook_size,"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1br-cagItt9S",
    "outputId": "ecdc439e-bcd0-4260-b2fb-f92224dbee17"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def decode_audio_bigcodec(\n",
    "    tokens,\n",
    "    quantizer,\n",
    "    n_original_tokens,\n",
    "    n_codebooks,\n",
    "):\n",
    "    # find audio start and end tokens\n",
    "    start, end = get_audio_start_end_tokens(\n",
    "        tokens, soa, eoa\n",
    "    )\n",
    "\n",
    "    # subtract length of original vocabulary -> tokens in range [0, 1024)\n",
    "    audio_tokens = tokens[start:end] % n_original_tokens\n",
    "    audio_tokens = audio_tokens.reshape(1, -1, 1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      emb = quantizer.decoder.vq2emb(audio_tokens).transpose(1, 2)\n",
    "      audio = quantizer.decoder(emb, vq=False).squeeze().detach().cpu()\n",
    "\n",
    "    del tokens\n",
    "    del audio_tokens\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return audio, 16000\n",
    "\n",
    "\n",
    "def infer_text_to_audio(text, model, tokenizer, quantizer, max_seq_length=1024, top_k=20, temperature=0.5):\n",
    "    text_tokenized = tokenizer(text.lower(), return_tensors=\"pt\")\n",
    "    text_input_tokens = text_tokenized[\"input_ids\"].to(device)\n",
    "\n",
    "    text_tokens = torch.cat([bos, text_input_tokens, soa], dim=1)\n",
    "    attention_mask = torch.ones(text_tokens.size(), device=device)\n",
    "\n",
    "    output_audio_tokens = model.generate(\n",
    "        text_tokens,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_seq_length,\n",
    "        top_k=top_k,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    audio_signal = decode_audio_bigcodec(\n",
    "        output_audio_tokens[0],\n",
    "        quantizer,\n",
    "        len(tokenizer),\n",
    "        n_codebooks_tts\n",
    "    )\n",
    "\n",
    "    return audio_signal\n",
    "\n",
    "\n",
    "def infer_audio_to_text(audio_path, model, tokenizer, quantizer, max_seq_length=1024, top_k=20, temperature=0.9):\n",
    "    audio_data, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "    audio = resample(audio_data, sample_rate)\n",
    "\n",
    "    codes = quantizer.encode(audio.reshape(1, -1))\n",
    "    raw_tokens = codes + len(tokenizer)\n",
    "    audio_tokens = raw_tokens[:n_codebooks_asr].view(1, -1)\n",
    "    tokens = torch.cat([bos, soa, audio_tokens, eoa], dim=1)\n",
    "    print(tokens)\n",
    "\n",
    "    audio, sr = decode_audio_bigcodec(tokens[0], quantizer, len(tokenizer), n_codebooks_asr)\n",
    "    torchaudio.save(\"decoded_audio.wav\", audio.unsqueeze(0), sr)\n",
    "\n",
    "    attention_mask = torch.ones(tokens.size(), device=device)\n",
    "\n",
    "    output_text_tokens = model.generate(\n",
    "        tokens,\n",
    "        attention_mask=attention_mask,\n",
    "        do_sample=True,\n",
    "        temperature=0.95,\n",
    "        top_k=100,\n",
    "        top_p=0.99,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    output_text_tokens = output_text_tokens.cpu()[0]\n",
    "    # output_text_tokens = output_text_tokens[output_text_tokens < soa]\n",
    "    decoded_text = tokenizer.decode(output_text_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return decoded_text\n",
    "\n"
   ],
   "metadata": {
    "id": "X_L85Ksstert"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text = \"count numbers from one to ten\"\n",
    "audio_signal, sr = infer_text_to_audio(text, model, tokenizer, quantizer, top_k=50, temperature=0.9)\n",
    "torchaudio.save(\"male_loud_count_numbers.wav\", audio_signal.unsqueeze(0), sr)\n"
   ],
   "metadata": {
    "id": "WRjOqCrNusOm"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "audio_path = \"/content/asr_test.wav\"\n",
    "text = infer_audio_to_text(audio_path, model, tokenizer, quantizer, top_k=50, temperature=1.0)\n",
    "print(text)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDPA6fZkxzK8",
    "outputId": "239336d4-277b-44d2-84c4-46158d4c9f9b"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Inout sample rate: 24000\n",
      "tensor([[151644, 151665, 157628, 156939, 152519, 153194, 157270, 156910, 152037,\n",
      "         153581, 158721, 155582, 156218, 155833, 157150, 156359, 154447, 155112,\n",
      "         156434, 158325, 158615, 154358, 156056, 155565, 156056, 158420, 155471,\n",
      "         154715, 159834, 155854, 151715, 155448, 153194, 154783, 156801, 153033,\n",
      "         157183, 152956, 152565, 152729, 154945, 152411, 156098, 158286, 159688,\n",
      "         152543, 153603, 155098, 157600, 158263, 158403, 156827, 159649, 153978,\n",
      "         156650, 154258, 155383, 156865, 158550, 157374, 156837, 155794, 154705,\n",
      "         155973, 157996, 159826, 152960, 158220, 153517, 156679, 156494, 157823,\n",
      "         154912, 153578, 158814, 159611, 152078, 152122, 158007, 154984, 158894,\n",
      "         153597, 155189, 157745, 157718, 159509, 156695, 159019, 158308, 152618,\n",
      "         153139, 151867, 155083, 153250, 154279, 154480, 156733, 158573, 157846,\n",
      "         157470, 159103, 156127, 153680, 156996, 156375, 159431, 155412, 153497,\n",
      "         157820, 158710, 153313, 154065, 152948, 153603, 152235, 156131, 156875,\n",
      "         156706, 156305, 155312, 151857, 157203, 152669, 159449, 154100, 155150,\n",
      "         155017, 155626, 158988, 156661, 158501, 151757, 155183, 158467, 158022,\n",
      "         153792, 157977, 159760, 151753, 151900, 156364, 157032, 156614, 156763,\n",
      "         153733, 155001, 157134, 156281, 159068, 153740, 152336, 157806, 155621,\n",
      "         154560, 157803, 155173, 158233, 152332, 158553, 154236, 157888, 152550,\n",
      "         155388, 159626, 157051, 159088, 155953, 156450, 151666]],\n",
      "       device='cuda:0')\n",
      " der roser musste sein. die räte war wirin.....??.!., uh........................... let's move on to see you...... you get some.... get some sort of sense. with............................................. sug....en....... löser..... was will. that's what we do.......... gern....... wunsch ich................................ the place....... little date........ and....... d. d........ gott.. in.................................... ......... waterde..... latin...... letwst.. r. r.. wow...... good moment............................................................................. a....... with the view of this..... was war..'s. was.......................................... and. politiker hat seine......................................................... is it, can..........?................................. let.... warum gehst..., ähm........ wo, war und war.....................?.... in-a.......?. man.. is......... wusst......a............... w.................................... hand.. is....... war........... jesse............... run..... was um........................ o......... g. burtel...est... w.......\n"
     ]
    }
   ]
  }
 ]
}
