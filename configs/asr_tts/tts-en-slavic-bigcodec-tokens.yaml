base_model: "Qwen/Qwen2.5-0.5B"
checkpoint_path: null
save_dir: "./results"

start_sequence_token: "<|im_start|>"
end_sequence_token: "<|im_end|>"
start_audio_token: "<|start_of_audio|>"
end_audio_token: "<|end_of_audio|>"

n_special_tokens: 3

data:
  - ksych/emilia_multilang_quantized
  - ksych/mozilla_slavic_quantized
  - ksych/librispeech_quantized

text_data: []

max_seq_length: 4096
raw_audio_length: 256000

path_to_cache: ".."

allow_tf32: True

filter_long_audio: False

tasks:
  - tts

# Quantizer settings
quantizer:
  speech:
    n_new_tokens: 0
  wav:
    n_new_tokens: 0
  bigcodec: 
    n_new_tokens: 8192
  asr: []

  tts:
    - quantizer: bigcodec
      n_codebooks: 1

# Training settings
train_batch_size: 2
eval_batch_size: 2
learning_rate: 1e-4
gradient_accumulation_steps: 4
lr_scheduler_type: "cosine"
num_train_epochs: 5
num_warmup_steps: 1000

weight_decay: 0.1
max_grad_norm: 0.25

torch_compile: False

# Logging settings
wandb_project_name: "vikhr4o-qwen2.5-0.5b"
